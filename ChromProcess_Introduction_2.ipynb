{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9446a720",
   "metadata": {},
   "source": [
    "# ChromProcess Introduction Part 2\n",
    "\n",
    "In ChromProcess Introduction Part 1, peak collections were created from chromatogram files. In Part 2, these peak tables are used to compile series data (e.g. changes in compound concentrations or peak integrals over time).\n",
    "\n",
    "Note that the results of each operation should be inspected or validated in some way before they are used to evaluate the 'final' data. The software may not be picking up important features of the data. Tuning the analysis parameters shown in this notebook may remedy issues, or further code must be included to achieve satisfactory results. It is up to the reader to choose a sufficient methods for validation and inspection of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c2b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ChromProcess import Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc266366",
   "metadata": {},
   "source": [
    "Peak collections can be loaded into objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64512f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChromProcess.Loading import peak_collection_from_csv\n",
    "\n",
    "peak_collection_directory = 'Sample_analysis/Example/ExperimentalData/ExamplePeakCollections'\n",
    "\n",
    "peak_tables = []\n",
    "for file in os.listdir(peak_collection_directory):\n",
    "    if file.endswith('.csv'):\n",
    "        peak_tables.append(peak_collection_from_csv(f'{peak_collection_directory}/{file}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed2546",
   "metadata": {},
   "source": [
    "The experiment conditions go hand-in-hand with the data, so they are loaded into an object. As well as an object containing calibration information for the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a52524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ChromProcess.Loading import analysis_from_csv\n",
    "from ChromProcess.Loading import conditions_from_csv\n",
    "\n",
    "conditions_file = 'Sample_analysis/Example/ExperimentalData/example_conditions.csv'\n",
    "analysis_file = 'Sample_analysis/Example/Analysis/example_analysis_details.csv'\n",
    "conditions = conditions_from_csv(conditions_file)\n",
    "analysis = analysis_from_csv(analysis_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cc7ff",
   "metadata": {},
   "source": [
    "The peak tables and experiment conditions are then used to create a series of peak collections in a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6e2b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.818\n",
      "8.001\n",
      "8.097\n",
      "10.396\n",
      "10.685\n",
      "10.854\n",
      "13.252\n",
      "13.26\n",
      "13.573\n",
      "13.578\n",
      "7.659\n",
      "7.817\n",
      "8.001\n",
      "8.098\n",
      "10.397\n",
      "10.688\n",
      "10.853\n",
      "13.259\n",
      "13.262\n",
      "13.576\n",
      "13.583\n",
      "7.815\n",
      "8.0\n",
      "8.096\n",
      "10.394\n",
      "10.682\n",
      "10.848\n",
      "13.251\n",
      "13.256\n",
      "13.573\n"
     ]
    }
   ],
   "source": [
    "# Create series of peak collections\n",
    "series = Classes.PeakCollectionSeries(\n",
    "                                    peak_tables, \n",
    "                                    name = 'Example',\n",
    "                                    conditions = conditions.conditions\n",
    "                                    )\n",
    "for p in series.peak_collections:\n",
    "    for pk in p.peaks:\n",
    "        print(pk.retention_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862eed",
   "metadata": {},
   "source": [
    "Now, there are a bunch of operations which can be performed on this series of peak collections. First, consider that similar peaks between chromatograms do not have *exactly* the same retention time. Some of this variability can be reduced by aligning each chromatogram according the the position of its internal standard. The internal standard retention time can be set to zero, and all other retention times are adjusted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d75735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.097\n",
      "8.28\n",
      "8.376\n",
      "10.675\n",
      "10.964\n",
      "11.133\n",
      "13.531\n",
      "13.539\n",
      "13.852\n",
      "13.857\n",
      "7.939\n",
      "8.097000000000001\n",
      "8.280999999999999\n",
      "8.378\n",
      "10.677\n",
      "10.968\n",
      "11.133\n",
      "13.539000000000001\n",
      "13.542000000000002\n",
      "13.856000000000002\n",
      "13.863\n",
      "8.095\n",
      "8.280000000000001\n",
      "8.376000000000001\n",
      "10.674\n",
      "10.962\n",
      "11.128\n",
      "13.530999999999999\n",
      "13.536000000000001\n",
      "13.853000000000002\n"
     ]
    }
   ],
   "source": [
    "from ChromProcess.Loading import instrument_cal_from_csv\n",
    "calibration = instrument_cal_from_csv('Sample_analysis/example_calibrations.csv')\n",
    "IS_pos = calibration.internal_standard_position\n",
    "series.align_peaks_to_IS(IS_pos)\n",
    "\n",
    "for p in series.peak_collections:\n",
    "    for pk in p.peaks:\n",
    "        print(pk.retention_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16230b",
   "metadata": {},
   "source": [
    "Next, the integrals of all peaks can be normalised to the internal standard's integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28112f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "series.reference_integrals_to_IS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591d933",
   "metadata": {},
   "source": [
    "If there are low-intensity peaks that are considered to be negligible (your decision), those below a certain integral value threshold can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbc0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_removal_limit = 0.05 # 5% of internal standard integral if integrals are normalised to IS\n",
    "series.remove_peaks_below_threshold(peak_removal_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d2d2a",
   "metadata": {},
   "source": [
    "The above operations clean up the data for finding a series for each peak through the set of peak collections. The following method uses a simple agglomerative clustering algorithm to create clusters of peak retention times. Each cluster will be used to identify which peaks will go into a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d90a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_agglomeration_boundary = 0.025 # distance cutoff \n",
    "series.get_peak_clusters(bound = peak_agglomeration_boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79d141",
   "metadata": {},
   "source": [
    "These clusters can then be used to output the series data for the peak integrals. First, a method is called to assemble the peak clusters into a numpy array. Then, a `DataReport` object is created from the `PeakCollectionSeries` object. The `DataReport` is then written to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb86a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_report_directory = 'Sample_analysis/Example/ExperimentalData/DataReports'\n",
    "\n",
    "series.write_data_reports(f'{data_report_directory}/{series.name}', analysis) # create arrays for output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa758c78",
   "metadata": {},
   "source": [
    "Peaks and clusters can also be assigned according to the retention time boundaries stored in the calibrations file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae258c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # assign peaks\n",
    "series.assign_peaks(calibration.boundaries)\n",
    "# assign clusters\n",
    "series.assign_clusters(calibration.boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c6b95",
   "metadata": {},
   "source": [
    "Following assignment of the peaks, a calibration function can then be applied to obtain concentrations using the calibration factors contained in the `InstrumentCalibration` object. Information from `AnalysisDetails`, such as dilution factors, are also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75cca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamrobinson/Documents/Software/ChromProcess/ChromProcess/Utils/utils/functions.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return factor*(-B + np.sqrt((B**2) - (4*A*(C-integ))))/(2*A)\n"
     ]
    }
   ],
   "source": [
    "# Apply calibrations to assigned peaks\n",
    "# (May give the error:\n",
    "# `RuntimeWarning: invalid value encountered in double_scalars`\n",
    "# due to multiplication by zero)\n",
    "series.apply_calibrations(analysis, calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7073e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for sample dilution\n",
    "series.apply_peak_dilution_factors(analysis.internal_standard_concentration, analysis.internal_standard_concentration_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71802e50",
   "metadata": {},
   "source": [
    "The results of calculating concentrations can then be output using a similar process as for the integral series. If peaks have ben asssigned, the assignments will be incorporated into the integral series file, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ddfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "series.write_data_reports(f'{data_report_directory}/{series.name}', analysis) # create arrays for output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1f40b",
   "metadata": {},
   "source": [
    "...and that's the end of Part 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromprocess-env",
   "language": "python",
   "name": "chromprocess-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
